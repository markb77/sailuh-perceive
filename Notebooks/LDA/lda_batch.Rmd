---
title: "LDA Model Batch"
author: "Carlos Paradis"
date: "July 4, 2017"
output:ins
  html_document:
    css: ~/OneDrive/Academia/Projects/inkCSS/res/css/style.css
    highlight: zenburn
    toc: yes
    toc_depth: 5
  word_document: default
---

# Introduction 

The purpose of this notebook is to show the code necessary to execute multiple LDA runs, and construct a topic flow across different topic-term matrix over monthly bins through cosine similarity. 

The data used consist of either Seclist's Full Disclosure or BugTraq mailing list, as output by the crawler module, and as such contains for every e-mail reply body a single file. Alternatively, the corpus may use the Aggregation module / data to merge the files into thread files. There is no assumption on the individual file content, other than it reflect the content of the e-mail reply (future work will consider removing e-mail signatures and performing other non-token oriented pre-processing).

```{r Create LDA Model}
source("~/Github/sail/perceive/Notebooks/LDA/lda_functions.R")

#fd.path <- "../Data/Full_Disclosure/2008/Body/"
#fd.path <- "/Users/carlos/Downloads/2013"
#bt.path <- "/Users/carlos/Downloads/2012"
#fd.path <- "/Users/carlos/Downloads/2013"
#fd.path <- "/Users/carlos/Downloads/estepona_2013_tf_corpus"
#fd.path <- "/Users/carlos/Desktop/2014"
fd.path <- "~/Desktop/new_crawler_fd_corpus/2013.parsed"
folder <- loadFiles(fd.path)
```

# LDA 

The VEM model is used, as implemented by Blei et al. Other models will be tested in a later stage. Model tunning is **not** performed in this notebook to showcase other ways to explore the data. Pre-processing parameters were discussed in an earlier stage of the research, but not exposed as parameters in model creation at this point, again, to simplify discussion and give room to the other functions available. As such, the choice of parameters here are simply k=10 for all months. 

```{r}
set.seed(1234)
models <- list()
models[["Jan"]] <- rawToLDA(folder,10,"Jan")
models[["Feb"]] <- rawToLDA(folder,10,"Feb")
models[["Mar"]] <- rawToLDA(folder,10,"Mar")
models[["Apr"]] <- rawToLDA(folder,10,"Apr")
models[["May"]] <- rawToLDA(folder,10,"May")
models[["Jun"]] <- rawToLDA(folder,10,"Jun")
models[["Jul"]] <- rawToLDA(folder,10,"Jul")
models[["Aug"]] <- rawToLDA(folder,10,"Aug")
models[["Sep"]] <- rawToLDA(folder,10,"Sep")
models[["Oct"]] <- rawToLDA(folder,10,"Oct")
models[["Nov"]] <- rawToLDA(folder,10,"Nov")
models[["Dec"]] <- rawToLDA(folder,10,"Dec")
```

```{r eval=FALSE}
# Export Document Term Matrix

names <- names(models)
#path <- "~/Desktop/2012_dtm_k_10_12"
path <- "~/Desktop/lda_export/Document_Topic_Matrix"
exportLDAModelAux <- function(name,models,path){
  lda.model <- models[[name]][["LDA"]]
  lda.model.posterior <- posterior(lda.model)
  dtm <- lda.model.posterior$topics
  path <- paste0(path,"/",name,".csv")
  
  write.csv(dtm,path,row.names=TRUE)
}
lapply(names,exportLDAModelAux,models,path)
```

```{r eval=FALSE}
# Export Topic Term Matrix

names <- names(models)
#path <- "~/Desktop/2012_ttm_k_10_12"
path <- "~/Desktop/lda_export/Topic_Term_Matrix"
exportLDAModelAux <- function(name,models,path){
  lda.model <- models[[name]][["LDA"]]
  lda.model.posterior <- posterior(lda.model)
  dtm <- lda.model.posterior$terms
  path <- paste0(path,"/",name,".csv")
  write.csv(dtm,path,row.names=TRUE)
}
lapply(names,exportLDAModelAux,models,path)

```

`models` is a named list after each month used to create a separate LDA model covering a single year. 

Each element of the list of models is a named list in itself containing the 3 most relevant data objects associated to the creation of the LDA Model:

 * `tokens`: Contains a matrix of tokens, which is used to create the document-term frequency matrix (dfm).
 * `dfm`: Contains the document-term frequency matrix, which is used as input to create the LDA model. 
 * `LDA`: Contains a class (as defined in package `topicmodels`) containing several parameters required to be defined a-priori for the model (e.g. the number of topics `k`), as well as the expected outputs of an LDA model: A `topic-term-matrix`, and a `document-topic-matrix`.

# Topic Exploration

There are varying ways to explore topics. Out of the surveyed methods available, Termite and LDAVis packages offered the best alternatives. We use LDAVis here to explore the topics **within a month**.  

## Topic-Term Matrix

```{r}
#2013: Feb, Apr, Dec
# To inspect a given topic use one of the following:
plotLDAVis(models[["Jan"]],as.gist=FALSE)
 ```

The visualization provides more information derived from the `topic-term-matrix` output by LDA. 

Additionally, we may also be interested in seeing how any one term is used across the corpus used for the specified month. For example, if the term `rpath` is of interest, we can inspect the documents, some of the content ocurring immediately before and after in it's original context, as so:

```{r}
View(kwic(models[["May"]][["tokens"]],"vulnerability")) #see context for token u since it is one of the words chosen to represent the topic
```

## Document-Topic Matrix

While LDAVis provides means to sort the topic terms beyond a maximum likelehood approach to understand which terms distinguish one topic of another through a sliding bar, and their ocurrence in respect to other topics (red and blue bars upon selection), it unfortunately falls short in providing visualization support to the raw content linkage available in the document-topic matrix. 

By using a maximum likelehood approach to transform the soft mapping matrix into a deterministic mapping, we can use it to guide exploration of the documents associated to the topic (in the future, a `nearest-likelehood-threshold` mechanism will be implemented to avoid documents with similar likelehood topics). For example, by observing the number of documents per topic:

```{r}
GetDocumentsPerTopicCount(models[["May"]][["LDA"]])
```

Or the name of the documents mapped to a specific topic to explore their content:

```{r}
GetDocumentsAssignedToTopicK(models[["Dec"]][["LDA"]],5)
  # Call quanteda library to mask View function if it doesn't show the table below on a browser
```

# Topic Flow 

The Topic Exploration functions are useful to look **within** a given month topics. However, up to this point no exploration or discussion is done concerning what happens between topics of **different months**. Specifically, we are interested in observing how topics change over time, i.e., the topic flow (future work will consider non-temporal relationships): 

```{r}
topic.flow <- CalculateTopicFlow(models)
View(topic.flow)
```

The table displays 2 sub-tables: To the left, Dec-Jan columns describes the topics of highest similarity of every month. The right-side sub-table provide the similarity between the pair of topics. For example, if Jan and Feb columns contain 2 and 3 in a given row to the left sub-table, then in the same row to the right sub-table in the column Jan_Feb will contain the similarity between the topics between months.

It is important to note the presented table contain a few assumptions in it's construction, in order to facilitate the visualization of the Topic Flow in a given year. 

Let's first motivate the assumptions: In this notebook, every month was defined as 10 topics. For every pair of months we wish to compare their similarity against one another, a total of 100 rows is created with the associated similarity. Since we are considering 12 months in every topic flow table (as shown above), we would have a total of 10^12 rows, clearly making it very hard for manual inspection. 

Instead, we resort again to a maximum likelehood approach: 

First, the January 10 topics is considered to be compared with the 10 topics of Febraury. Rather than showing 100 topics, we choose only those with the maximum likelehood similarity from Feb, as such, keeping the number of rows to 10, rather than 100 (note a shortcoming here occurs when other topics from Feb have similar similarity to the highest similarity topic, as discussed before). 

**Independendly**, we apply the same algorithm for **Feb_Mar**. Notice because of this, we consider all topics of Feb here on every row, and have on the Mar column only the maximum likelehood months of March. 

Notice this looks apriori inconsistent: In **Jan_Feb** we only have the topics of **Feb** that were maximum likelehood for at least one topic of **Jan**. In turn, the **Feb_Mar** pair will contain **all topics of Feb**.

We emphasize again the two pargraphs above indicate 2 **independent** operations: We could have calculated in parallel the maximimum likelehood between **Jan_Feb**, and **Feb_Mar**, and all the other pair of months if we wanted to speed up the operation. The "gluing" across all pair of months tables is done afterwards. 

Understanding this operation as independent is important to understand how `NA`s are introduced in the table. A intuitive follow up to glue, for example, the **Jan_Feb** table and the **Feb_Mar** table would be a simple `inner join` through the **common month**, in this example **Feb**. However, we would miss the opportunity to observe **Emerging**, **Continuing**, **Ending** and **Standalone** topic types.

The idea behind labeling topics as of different types is borrowed from Smith et al (2015), in their proposal of Topic Flow. In the original work, and here, the type labels are assigned to a Topic based on their connectivity between months. Because the topics are connected as cartesian products through probabilities, all topics a priori are connected to each other. It is only when we apply some heuristic to reason this "probabilisitic link" as "Yes/No" that types makes sense, or in other words, make the links deterministic. 

In Smith et al. (2015), this is done using thresholds: A sliding bar is used to filter out all links that falls below a given threshold. In our approach, this happens through the maximum likelehood assumption: Topics in a follow-up month may not have links if none of the topics in the current month had it ocurring as highest similarity.

As such, we can apply the same characterization here, by performing an **full outer join** in tables **Jan_Feb** and **Feb_Mar**. 
 
 * **Continuing**: A topic

The table used is a different approach than that used by Topic Flow visualization, also used by the project this notebook is part of.

# Topic Flow for Leximancer

Leximancer is an (paid) alternative to LDA, and contains already as part of it's suite a visualization that replaces LDAVis. The code below provides a transformation for it's equivalent to topic-term matrix in order to construct topic flows in the same way as done as LDA. This section, although functional, is still undergoing work as the tool as only recently acquired by the group. 

```{r}
l.ttm1 <- fread("~/Desktop/ttm_leximancer_2009.csv")
l.ttm1.names <- l.ttm1$concept
l.ttm1 <- l.ttm1[,2:ncol(l.ttm1),with=FALSE]
l.ttm1 <- as.matrix(l.ttm1)

l.ttm2 <- fread("~/Desktop/ttm_leximancer_2010.csv")
l.ttm2.names <- l.ttm2$concept
l.ttm2 <- l.ttm2[,2:ncol(l.ttm2),with=FALSE]
l.ttm2 <- as.matrix(l.ttm2)
similarity <- CalculateHighestTopicCosineSimilarity(l.ttm1,l.ttm2)
write.csv(similarity,"~/Desktop/leximancer_FD_similarity_2009_2010.csv",row.names=FALSE)

# Beware this is a over simplification of the topic, using the highest likelehood word rather than all it's terms:
similarity$ttm1 <- l.ttm1.names[similarity$ttm1]
similarity$ttm2 <- l.ttm2.names[similarity$ttm2]
write.csv(similarity,"~/Desktop/leximancer_FD_similarity_highest_likelehood_term_representation_2009_2010.csv",row.names=FALSE)
```

# Other Visualizations

Another option for visualizing the data is using word clouds, although the method doesn't provide much insight into the data when compared to the others presented in this notebook. It is only stated here for documentation purposes. 

```{r Plots, eval=FALSE}
textplot_wordcloud(tfidf(dfm2), min.freq = 2000, 1random.order = FALSE,rot.per = 0,colors = RColorBrewer::brewer.pal(8,"Dark2"))
```

